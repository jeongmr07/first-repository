{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIFFEL 1st Project : 가위바위보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 데이터 Resize  \n",
    "- 원 데이터의 사이즈를 줄여주는 과정 (224x224 > 28x28)  \n",
    "- rock_scissor_paper 폴더 안의 모든 파일을 읽어 사이즈를 바꾸어 다시 저장한다.  \n",
    "    - for 문으로 바꿀 수 없나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 디렉토리 경로:  /home/aiffel/aiffel/train/rock_scissor_paper*/scissor\n",
      "이미지 디렉토리 경로:  /home/aiffel/aiffel/train/rock_scissor_paper*/rock\n",
      "이미지 디렉토리 경로:  /home/aiffel/aiffel/train/rock_scissor_paper*/paper\n",
      "모든 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "# PIL 라이브러리가 설치되어 있지 않다면 설치\n",
    "# !pip install pillow   \n",
    "\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/train/rock_scissor_paper*/scissor\"\n",
    "print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "\n",
    "images=glob.glob(image_dir_path + \"/*.jpg\")  \n",
    "\n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img.save(img,\"JPEG\")\n",
    "    \n",
    "# 바위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/train/rock_scissor_paper*/rock\"\n",
    "print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "\n",
    "images=glob.glob(image_dir_path + \"/*.jpg\") \n",
    "\n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img.save(img,\"JPEG\")\n",
    "\n",
    "# 보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/train/rock_scissor_paper*/paper\"\n",
    "print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "\n",
    "images=glob.glob(image_dir_path + \"/*.jpg\") \n",
    "\n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "target_size=(28,28)\n",
    "for img in images:\n",
    "    old_img=Image.open(img)\n",
    "    new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "    new_img.save(img,\"JPEG\")\n",
    "\n",
    "print(\"모든 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. 가위바위보 이미지 불러오기 _ train set  \n",
    "- 처음 저장해두었던 train 데이터는 300개 (현재는 2100개) 의 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(img_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=300   # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/train/rock_scissor_paper_0\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 확인하기 : 300장이므로 0~299 인덱스만 사용가능\n",
    "plt.imshow(x_train[299],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. 모델 만들고 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# model을 직접 만들어 보세요.\n",
    "# 하이퍼파라미터 설정\n",
    "n_channel_1=16\n",
    "n_channel_2=20\n",
    "n_dense=30\n",
    "n_train_epoch=35\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3))) # 컬러 이미지 ; shape 28x28x3\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax')) # 데이터 라벨 3개 ; output 3개!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 27.0520 - accuracy: 0.3333\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 5.2681 - accuracy: 0.4467\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 2.3020 - accuracy: 0.5200\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.4205 - accuracy: 0.6667\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.7722 - accuracy: 0.7533\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.8533\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8867\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.9233\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1480 - accuracy: 0.9567\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9633\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1891 - accuracy: 0.9167\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1463 - accuracy: 0.9367\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.9367\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9833\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0320 - accuracy: 0.9933\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9900\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9967\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0149 - accuracy: 0.9967\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fda007d0e50>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model을 학습시키는 코드\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train, y_train, epochs=n_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <U>과적합(overfitting)</U>이 발생한 것으로 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. 테스트 이미지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "# x_test, y_test를 만드는 방법은 x_train, y_train을 만드는 방법과 아주 유사합니다.\n",
    "# 파일 불러오기\n",
    "def load_data(img_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=300   # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"데이터의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/test/rock_scissor_paper\"\n",
    "(x_test_s, y_test_s)=load_data(image_dir_path)\n",
    "x_test_norm = x_test_s/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test_s.shape))\n",
    "print(\"y_test shape: {}\".format(y_test_s.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 17.4621 - accuracy: 0.4000\n",
      "test_loss: 17.46211814880371 \n",
      "test_accuracy: 0.4000000059604645\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼파라미터(n_channel_1, n_channel_2, n_dense, n_train_epoch)를 조절해도 테스트 정확도가 30% 안팎으로 도출되어 ***학습데이터에 문제가 있다*** 판단.\n",
    "> 학습데이터 수가 적다.  \n",
    "> 학습데이터의 퀄리티가 좋지 않다.\n",
    "\n",
    "AIFFEL 학생분들의 데이터를 모아 다시 학습시켜보려 한다. 다운받은 데이터를 train 폴더에 넣은 후 다시 모델 학습하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. 다시 데이터 모으기\n",
    "- 다운로드한 데이터를 train set 폴더에 넣었다. 코드 수정을 적게 하려고 rock_scissor_paper 공통된 이름의 폴더의 데이터만 추가했다.  \n",
    "    - 이름 상관없이 데이터 추가하는 법이 있는지 찾아보기  \n",
    "- 데이터 개수가 달라 load_data_train으로 함수 이름을 수정하였다.  \n",
    "    - 데이터 개수 상관없이 불러 올 수 있는 방법은 없나?\n",
    "    \n",
    "<img src=\"Pictures/trainingset.png\" width=\"700px\" height=\"400px\"></img><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 3300 입니다.\n",
      "x_train shape: (3300, 28, 28, 3)\n",
      "y_train shape: (3300,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data_train(img_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=3300   # 가위바위보 이미지 개수 총합에 주의하세요. 모은 데이터 개수\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/train/rock_scissor_paper*\"\n",
    "(x_train, y_train)=load_data_train(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. 새로운 학습 데이터로 모델 학습 및 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 2.9532 - accuracy: 0.5224\n",
      "Epoch 2/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6200 - accuracy: 0.7409\n",
      "Epoch 3/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4099 - accuracy: 0.8409\n",
      "Epoch 4/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2844 - accuracy: 0.8927\n",
      "Epoch 5/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2190 - accuracy: 0.9209\n",
      "Epoch 6/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9458\n",
      "Epoch 7/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9527\n",
      "Epoch 8/25\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 0.1127 - accuracy: 0.9633\n",
      "Epoch 9/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9476\n",
      "Epoch 10/25\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9739\n",
      "Epoch 11/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9742\n",
      "Epoch 12/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0560 - accuracy: 0.9848\n",
      "Epoch 13/25\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 0.0911 - accuracy: 0.9697\n",
      "Epoch 14/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0514 - accuracy: 0.9815\n",
      "Epoch 15/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0328 - accuracy: 0.9918\n",
      "Epoch 16/25\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.9930\n",
      "Epoch 17/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9733\n",
      "Epoch 18/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0462 - accuracy: 0.9876\n",
      "Epoch 19/25\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9758\n",
      "Epoch 20/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9682\n",
      "Epoch 21/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9800\n",
      "Epoch 22/25\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 0.0424 - accuracy: 0.9855\n",
      "Epoch 23/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9852\n",
      "Epoch 24/25\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.0142 - accuracy: 0.9961\n",
      "Epoch 25/25\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f548878a150>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# model을 직접 만들어 보세요. 16, 32, 30, 20\n",
    "n_channel_1=16\n",
    "n_channel_2=48\n",
    "n_dense=50\n",
    "n_train_epoch=25\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3))) # 컬러 이미지 ; shape 28x28x3\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax')) # 데이터 라벨 3개 ; output 3개!\n",
    "\n",
    "# model을 학습시키는 코드\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train, y_train, epochs=n_train_epoch)\n",
    "\n",
    "# 모델 평가 : 데이터 300 개\n",
    "# test_loss, test_accuracy = model.evaluate(x_test_s,y_test_s, verbose=2)\n",
    "# print(\"test_loss: {} \".format(test_loss))\n",
    "# print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전보다 나아진 것을 볼 수 있다.   \n",
    "하지만  \n",
    "> 하이퍼파라미터 조절마다 발생하는 정확도 차이.  \n",
    "> 트레이닝 중, 정확도의 작은 변동이 있다.  \n",
    "\n",
    "라고 생각되어 테스트 데이터를 덧붙여 평가를 시도했다.  \n",
    "학습데이터셋과 테스트데이터셋의 개수 차이가 있어 함수를 다르게 두어(load_data_test) 데이터를 불러왔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 이미지 개수는 600 입니다.\n",
      "x_test shape: (600, 28, 28, 3)\n",
      "y_test shape: (600,)\n"
     ]
    }
   ],
   "source": [
    "# x_test, y_test를 만드는 방법은 x_train, y_train을 만드는 방법과 아주 유사합니다.\n",
    "# 파일 불러오기\n",
    "def load_data_test(img_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=600   # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"데이터의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/test/rock_scissor_paper*\"\n",
    "(x_test, y_test)=load_data_test(image_dir_path)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
      "test_loss: 0.002138259122148156 \n",
      "test_accuracy: 1.0\n",
      "19/19 - 1s - loss: 0.2672 - accuracy: 0.9467\n",
      "test_loss: 0.26716357469558716 \n",
      "test_accuracy: 0.9466666579246521\n"
     ]
    }
   ],
   "source": [
    "# 300 개의 테스트 데이터\n",
    "test_loss, test_accuracy = model.evaluate(x_test_s,y_test_s, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))\n",
    "\n",
    "# 600 개의 테스트 데이터\n",
    "test_loss, test_accuracy = model.evaluate(x_test,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터가 많을 수록 정확도는 점차 떨어지는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마무리\n",
    "2020.01.06.WED\n",
    "- 300장의 학습 데이터을 바탕으로 한 모델 학습에서 하이퍼파라미터 조절 시, 자칫 과적합(overfitting) 정도를 파악하기 힘들었다. 조금의 조절에서도 과적합으로 보이는 결과가 나왔다. 과적합이 아닌 경우에도 학습만 잘 될 뿐, ***모델 성능 테스트에서 3-40%의 낮은 성능의 결과***를 낳았다. 하이퍼파라미터 조절로도 50% 근방의 성능도 얻지 못했다. <u>이로서, 학습 데이터에 문제가 있다고 보고 AIFFEL 학생분들이 공유해주신 데이터를 모아 모델 학습과 테스트를 다시 진행하였다.</u>\n",
    "- 다시 진행해 본 과정에서 **학습 데이터 2100개와 테스트 데이터 300개**로 진행하였다. 모델을 만드는 과정은 똑같이 수행하고, 하이퍼파라미터만 조절했다. 전 과정보다 과적합의 경우가 현저히 줄었다. 학습 중 꾸준히 증가세를 보이긴 하지만 2% 안팎의 학습 성능 변동이 있었다. 학습된 모델로 테스트를 진행하였을 때, **100%**의 성능을 얻었지만! 모델이 좋아서 성능이 좋다고 판단하기 힘들었다.\n",
    "- 테스트 데이터가 더 많아지면 테스트 성능도 떨어질 것이라는 예상으로 **테스트 테이터 600개**로 같은 과정을 진행했다. 좋은 모델이라면 성능 차이가 그렇게 크지 않을 것이라 생각했으나, loss는 100배 더 크게, 성능은 약 6% 가량 낮은 결과를 보였다. 하이퍼파라미터를 더 조절해 테스트 개수가 많더라도 성능 차이를 줄일 수 있을 것이다. 하지만 학습 데이터가 많을 수록 모델 학습 및 성능이 좋아지는 것은 분명하다.\n",
    "    - 현재 하이퍼파라미터; n_channel_1=16, n_channel_2=48, n_dense=50, n_train_epoch=25\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
